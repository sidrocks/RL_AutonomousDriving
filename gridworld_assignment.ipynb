{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GridWorld Policy Evaluation Assignment\n",
                "\n",
                "## Objective\n",
                "Given a 4x4 GridWorld where an agent starts at (0,0) and tries to reach (3,3), calculate the value function $V(s)$ for a random policy using Iterative Policy Evaluation.\n",
                "\n",
                "- **Grid Size**: 4x4\n",
                "- **Actions**: Up, Down, Left, Right (prob = 0.25 each)\n",
                "- **Rewards**: -1 per step, 0 at terminal state (3,3)\n",
                "- **Gamma**: 1.0 (No discounting)\n",
                "- **Threshold**: 1e-4\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Configuration\n",
                "N = 4\n",
                "gamma = 1.0\n",
                "theta = 1e-4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions\n",
                "Define the environment logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def is_terminal(state):\n",
                "    return state == (N-1, N-1)\n",
                "\n",
                "def step(state, action):\n",
                "    # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
                "    r, c = state\n",
                "    if action == 0: r -= 1\n",
                "    if action == 1: r += 1\n",
                "    if action == 2: c -= 1\n",
                "    if action == 3: c += 1\n",
                "    \n",
                "    # Boundary Check: Stay in same place if hitting wall\n",
                "    if r < 0 or r >= N or c < 0 or c >= N:\n",
                "        r, c = state\n",
                "        \n",
                "    return (r, c)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Bellman Equation\n",
                "\n",
                "We use the **Bellman Expectation Equation** for a specific policy $\\pi$ to iteratively update the value of each state $s$. The equation is given by:\n",
                "\n",
                "$$V_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')]$$\n",
                "\n",
                "Where:\n",
                "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ (0.25 for all actions).\n",
                "- $P(s'|s,a)$ is the transition probability (1.0 for deterministic moves).\n",
                "- $R(s,a,s')$ is the reward (-1 for all transitions).\n",
                "- $\\gamma$ is the discount factor (1.0).\n",
                "- $V_k(s')$ is the value of the next state from the previous iteration.\n",
                "\n",
                "This update rule averages the expected returns from all possible future states."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Iterative Policy Evaluation\n",
                "Run the loop until convergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Converged in 471 iterations\n",
                        "Final Value Function:\n",
                        "[[-59.42367735 -57.42387125 -54.2813141  -51.71012579]\n",
                        " [-57.42387125 -54.56699476 -49.71029394 -45.13926711]\n",
                        " [-54.2813141  -49.71029394 -40.85391609 -29.99766609]\n",
                        " [-51.71012579 -45.13926711 -29.99766609   0.        ]]\n"
                    ]
                }
            ],
            "source": [
                "V = np.zeros((N, N))\n",
                "actions = [0, 1, 2, 3]\n",
                "prob = 0.25\n",
                "\n",
                "iteration = 0\n",
                "while True:\n",
                "    delta = 0\n",
                "    V_new = np.copy(V)\n",
                "    \n",
                "    for r in range(N):\n",
                "        for c in range(N):\n",
                "            state = (r, c)\n",
                "            if is_terminal(state):\n",
                "                continue\n",
                "                \n",
                "            v = V[r, c]\n",
                "            new_val = 0\n",
                "            \n",
                "            # Bellman Equation Update\n",
                "            for a in actions:\n",
                "                next_state = step(state, a)\n",
                "                reward = -1\n",
                "                next_r, next_c = next_state\n",
                "                \n",
                "                new_val += prob * (reward + gamma * V[next_r, next_c])\n",
                "            \n",
                "            V_new[r, c] = new_val\n",
                "            delta = max(delta, abs(v - new_val))\n",
                "            \n",
                "    V = V_new\n",
                "    iteration += 1\n",
                "    \n",
                "    if delta < theta:\n",
                "        print(f\"Converged in {iteration} iterations\")\n",
                "        break\n",
                "\n",
                "print(\"Final Value Function:\")\n",
                "print(V)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Local)",
            "language": "python",
            "name": "local_python"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
